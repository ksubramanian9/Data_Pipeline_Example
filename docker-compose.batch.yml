services:
  data-generator:
    image: python:3.11-slim
    container_name: batch-data-generator
    restart: "no"
    working_dir: /opt/services/batch
    entrypoint:
      - python
      - generate_synthetic_data.py
    command:
      - --config
      - /opt/services/batch/schemas/card_transactions.json
      - --output-dir
      - /opt/spark-data/input
      - --output-name
      - transactions
      - --clear-output
    volumes:
      - ./services:/opt/services:ro
      - ./data:/opt/spark-data

  namenode:
    image: apache/hadoop:${HADOOP_VERSION}
    container_name: namenode
    user: "0:0"
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - ./hdfs/namenode:/hadoop/dfs/name
      - ./data:/opt/spark-data
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      mkdir -p /hadoop/dfs/name &&
      chown -R root:root /hadoop/dfs/name &&
      if [ ! -d /hadoop/dfs/name/current ] || [ -z \"$(ls -A /hadoop/dfs/name 2>/dev/null)\" ]; then
        echo 'Formatting NameNode (first run)...';
        hdfs namenode -format -nonInterActive -force || true;
      fi &&
      exec hdfs namenode
      "
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -sf http://localhost:9870 >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20

  datanode:
    image: apache/hadoop:${HADOOP_VERSION}
    container_name: datanode
    user: "0:0"
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - ./hdfs/datanode:/hadoop/dfs/data
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      mkdir -p /hadoop/dfs/data &&
      chown -R root:root /hadoop/dfs/data &&
      exec hdfs datanode
      "

  hdfs-init:
    image: apache/hadoop:${HADOOP_VERSION}
    container_name: hdfs-init
    depends_on:
      data-generator:
        condition: service_completed_successfully
      namenode:
        condition: service_healthy
    user: "0:0"
    environment:
      - HADOOP_USER_NAME=root
    volumes:
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./data:/opt/spark-data
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      echo 'Waiting for HDFS...';
      until hdfs dfs -ls / >/dev/null 2>&1; do sleep 3; done;
      echo 'Creating /data/input and /data/output in HDFS...';
      hdfs dfs -mkdir -p /data/input /data/output;
      echo 'Creating /spark-events in HDFS...';
      hdfs dfs -mkdir -p /spark-events;
      hdfs dfs -chmod -R 1777 /data /spark-events;
      if compgen -G '/opt/spark-data/input/*' > /dev/null; then
        echo 'Seeding host input files into HDFS /data/input ...';
        hdfs dfs -put -f /opt/spark-data/input/* /data/input/;
      else
        echo 'No host seed files found under ./data/input â€“ skipping seed.';
      fi;
      echo 'HDFS init complete.'
      "
    restart: "no"

  spark-master:
    image: apache/spark:${SPARK_VERSION}
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      exec /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master \
        --host spark-master \
        --port 7077 \
        --webui-port 8080
      "

  spark-worker-1:
    image: apache/spark:${SPARK_VERSION}
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - --webui-port
      - "8081"
    command:
      - spark://spark-master:7077

  spark-worker-2:
    image: apache/spark:${SPARK_VERSION}
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8082:8082"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - --webui-port
      - "8082"
    command:
      - spark://spark-master:7077

  spark-history-server:
    image: apache/spark:${SPARK_VERSION}
    container_name: spark-history-server
    depends_on:
      - spark-master
    ports:
      - "18081:18081"
    volumes:
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /bin/bash
      - -lc
    command: >
      SPARK_HISTORY_OPTS='-Dspark.history.fs.logDirectory=hdfs://namenode:8020/spark-events' \
      exec /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer

  spark-app:
    image: apache/spark:${SPARK_VERSION}
    container_name: spark-app
    depends_on:
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - hdfs-init
    environment:
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - SCHEMA_CONFIG_PATH=/opt/services/batch/schemas/card_transactions.json
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /bin/bash
      - -lc
      - |
          echo 'Launching Spark batch job (input readiness handled in code)...';
          exec /opt/spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --conf spark.eventLog.enabled=true \
            --conf spark.eventLog.dir=hdfs://namenode:8020/spark-events \
            /opt/services/batch/pipeline_batch.py

  dashboard:
    build: ./dashboard
    container_name: retail-dashboard
    depends_on:
      - spark-app
    ports:
      - "5000:5000"
    environment:
      - OUTPUT_DIR=/opt/spark-data/output
      - SCHEMA_CONFIG_PATH=/opt/services/batch/schemas/card_transactions.json
    volumes:
      - ./data:/opt/spark-data
