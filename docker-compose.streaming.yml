services:
  namenode:
    image: apache/hadoop:3.3.6
    container_name: namenode
    user: "0:0"
    ports:
      - "9870:9870"
      - "8020:8020"
    volumes:
      - ./hdfs/namenode:/hadoop/dfs/name
      - ./data:/opt/spark-data
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      mkdir -p /hadoop/dfs/name &&
      chown -R root:root /hadoop/dfs/name &&
      if [ ! -d /hadoop/dfs/name/current ] || [ -z \"$(ls -A /hadoop/dfs/name 2>/dev/null)\" ]; then
        echo 'Formatting NameNode (first run)...';
        hdfs namenode -format -nonInterActive -force || true;
      fi &&
      exec hdfs namenode
      "
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -sf http://localhost:9870 >/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 20

  datanode:
    image: apache/hadoop:3.3.6
    container_name: datanode
    user: "0:0"
    depends_on:
      - namenode
    ports:
      - "9864:9864"
    volumes:
      - ./hdfs/datanode:/hadoop/dfs/data
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      mkdir -p /hadoop/dfs/data &&
      chown -R root:root /hadoop/dfs/data &&
      exec hdfs datanode
      "

  hdfs-init:
    image: apache/hadoop:3.3.6
    container_name: hdfs-init
    depends_on:
      namenode:
        condition: service_healthy
    user: "0:0"
    environment:
      - HADOOP_USER_NAME=root
    volumes:
      - ./conf/core-site.xml:/opt/hadoop/etc/hadoop/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/hadoop/etc/hadoop/hdfs-site.xml:ro
      - ./data:/opt/spark-data
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      echo 'Waiting for HDFS...';
      until hdfs dfs -ls / >/dev/null 2>&1; do sleep 3; done;
      echo 'Creating /data/input and /data/output in HDFS...';
      hdfs dfs -mkdir -p /data/input /data/output;
      echo 'Creating /spark-events in HDFS...';
      hdfs dfs -mkdir -p /spark-events;
      hdfs dfs -chmod -R 1777 /data /spark-events;
      if compgen -G '/opt/spark-data/input/*' > /dev/null; then
        echo 'Seeding host input files into HDFS /data/input ...';
        hdfs dfs -put -f /opt/spark-data/input/* /data/input/;
      else
        echo 'No host seed files found under ./data/input â€“ skipping seed.';
      fi;
      echo 'HDFS init complete.'
      "
    restart: "no"

  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.1
    container_name: zk
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.6.1
    container_name: kafka
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zk:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    ports:
      - "9092:9092"
      - "29092:29092"

  spark-master:
    image: apache/spark:latest
    container_name: spark-master
    depends_on:
      - namenode
      - datanode
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8080:8080"
      - "7077:7077"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint: ["/bin/bash", "-lc"]
    command: >
      "
      exec /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master \
        --host spark-master \
        --port 7077 \
        --webui-port 8080
      "

  spark-worker-1:
    image: apache/spark:latest
    container_name: spark-worker-1
    depends_on:
      - spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8081:8081"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - --webui-port
      - "8081"
    command:
      - spark://spark-master:7077

  spark-worker-2:
    image: apache/spark:latest
    container_name: spark-worker-2
    depends_on:
      - spark-master
    environment:
      - SPARK_NO_DAEMONIZE=true
    ports:
      - "8082:8082"
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /opt/spark/bin/spark-class
      - org.apache.spark.deploy.worker.Worker
      - --webui-port
      - "8082"
    command:
      - spark://spark-master:7077

  spark-history-server:
    image: apache/spark:latest
    container_name: spark-history-server
    depends_on:
      - spark-master
    ports:
      - "18081:18081"
    volumes:
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /bin/bash
      - -lc
    command: >
      SPARK_HISTORY_OPTS='-Dspark.history.fs.logDirectory=hdfs://namenode:8020/spark-events' \
      exec /opt/spark/bin/spark-class org.apache.spark.deploy.history.HistoryServer

  spark-stream:
    image: apache/spark:latest
    container_name: spark-stream
    depends_on:
      - kafka
      - spark-master
      - spark-worker-1
      - spark-worker-2
      - hdfs-init
    environment:
      - PYSPARK_PYTHON=python3
      - PYSPARK_DRIVER_PYTHON=python3
      - STREAM_OUTPUT_PATH=file:///opt/spark-data/output/streaming_product_revenue
      - STREAM_CHECKPOINT_DIR=/opt/spark-checkpoints/streaming_sales
      - SPARK_IVY_DIR=/tmp/spark-ivy
    volumes:
      - ./services:/opt/services
      - ./data:/opt/spark-data
      - ./checkpoints:/opt/spark-checkpoints
      - ./spark-events:/tmp/spark-events
      - ./conf/core-site.xml:/opt/spark/conf/core-site.xml:ro
      - ./conf/hdfs-site.xml:/opt/spark/conf/hdfs-site.xml:ro
    entrypoint:
      - /bin/bash
      - -lc
      - |
          echo 'Launching Spark streaming job (Kafka -> HDFS aggregates)...';
          IVY_DIR="$${SPARK_IVY_DIR:-/tmp/.ivy2}"
          if [ -z "$${IVY_DIR}" ]; then
            IVY_DIR="/tmp/.ivy2"
          fi
          if ! mkdir -p "$${IVY_DIR}/cache" "$${IVY_DIR}/jars"; then
            echo "Failed to create Ivy cache directories under $${IVY_DIR}" >&2
            IVY_DIR="/tmp/.ivy2"
            echo "Falling back to default Ivy cache directory $${IVY_DIR}" >&2
            if ! mkdir -p "$${IVY_DIR}/cache" "$${IVY_DIR}/jars"; then
              echo "Unable to create fallback Ivy cache directories under $${IVY_DIR}" >&2
              exit 1
            fi
          fi
          echo "Using Ivy cache directory $${IVY_DIR}"
          exec /opt/spark/bin/spark-submit \
            --master spark://spark-master:7077 \
            --conf spark.eventLog.enabled=true \
            --conf spark.eventLog.dir=hdfs://namenode:8020/spark-events \
            --conf spark.jars.ivy=$${IVY_DIR} \
            --packages $${SPARK_KAFKA_PACKAGE:-org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1} \
            /opt/services/streaming/streaming_sales_aggregator.py

  event-generator:
    build:
      context: .
      dockerfile: services/event-generator/Dockerfile
    container_name: event-generator
    depends_on:
      - kafka
      - hdfs-init
    environment:
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - KAFKA_TOPIC=sales
      - EVENTS_PER_SECOND=5
      - EVENT_INPUT_DIR=/opt/spark-data/input
    volumes:
      - ./data:/opt/spark-data
    command:
      - --loop
      - --shuffle

  streaming-dashboard:
    build:
      context: ./streaming_dashboard
      dockerfile: Dockerfile
    container_name: streaming-dashboard
    depends_on:
      - spark-stream
    environment:
      - STREAM_PARQUET_DIR=/opt/spark-data/output/streaming_product_revenue
    volumes:
      - ./data:/opt/spark-data
    ports:
      - "5100:5100"
